{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T06:24:37.380692Z",
     "start_time": "2018-10-17T06:24:37.376417Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import ipympl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T10:32:33.216461Z",
     "start_time": "2018-10-17T10:32:33.194687Z"
    }
   },
   "outputs": [],
   "source": [
    "reward_mtx = np.load('./results/reward_matrix_dqn_19_56.dat')\n",
    "reward_mtx_lr_decreasing = np.load('./results/reward_matrix_14_55_16_8_decreasing_lr.dat') # deeper nn architecture\n",
    "reward_mtx_dueling = np.load('./results/reward_matrix_17_12_dueling.dat')\n",
    "reward_mtx_dqn_lr_const = np.load('./results/reward_matrix_dqn_no_lr_21_30.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T10:33:04.688385Z",
     "start_time": "2018-10-17T10:33:04.299490Z"
    }
   },
   "outputs": [],
   "source": [
    "cum_reward_mtx = np.zeros_like(reward_mtx)\n",
    "cum_reward_mtx_lr_decreasing = np.zeros_like(reward_mtx_lr_decreasing)\n",
    "cum_reward_mtx_dueling = np.zeros_like(reward_mtx_dueling)\n",
    "cum_reward_mtx_lr_const = np.zeros_like(reward_mtx_dqn_lr_const)\n",
    "for i in range(reward_mtx.shape[1]):\n",
    "    cum_reward_mtx[:, i] = np.sum(reward_mtx[:, 0:i], axis=1)\n",
    "    cum_reward_mtx_lr_decreasing[:, i] = np.sum(reward_mtx_lr_decreasing[:, 0:i], axis=1)\n",
    "    cum_reward_mtx_dueling[:, i] = np.sum(reward_mtx_dueling[:, 0:i], axis=1)\n",
    "    cum_reward_mtx_lr_const[:, i] = np.sum(reward_mtx_dqn_lr_const[:, 0:i], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T10:33:49.440224Z",
     "start_time": "2018-10-17T10:33:49.435664Z"
    }
   },
   "outputs": [],
   "source": [
    "reward_mtx_avg = np.convolve(cum_reward_mtx[:, -1], np.ones((100,))/100, mode='valid')\n",
    "reward_mtx_lr_decreasing_avg = np.convolve(cum_reward_mtx_lr_decreasing[:, -1], np.ones((100,))/100, mode='valid')\n",
    "reward_mtx_dueling_avg = np.convolve(cum_reward_mtx_dueling[:, -1], np.ones((100,))/100, mode='valid')\n",
    "reward_mtx_dqn_lr_const_avg = np.convolve(cum_reward_mtx_lr_const[:, -1], np.ones((100,))/100, mode='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
    "\n",
    "for i in range(15):\n",
    "    state = env.reset()\n",
    "    for j in range(200):\n",
    "        action = agent.choose_action(state, 0.0)\n",
    "        env.render()\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            break \n",
    "            \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T10:35:11.247364Z",
     "start_time": "2018-10-17T10:35:11.206816Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/drlnd/lib/python3.6/site-packages/matplotlib/pyplot.py:522: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2357d01e820247ad86fe5996c00e11f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "#plt.plot(cum_reward_mtx_dueling[:, -1],'b', alpha=0.1)\n",
    "#plt.plot(reward_mtx_dueling_avg, 'b:')\n",
    "#plt.plot(cum_reward_mtx_lr_decreasing[:, -1],'k', alpha=0.1)\n",
    "#plt.plot(reward_mtx_lr_decreasing_avg, 'k:')\n",
    "#plt.plot(cum_reward_mtx[:, -1],'r', alpha=0.1)\n",
    "plt.plot(reward_mtx_avg, 'r:')\n",
    "plt.plot(reward_mtx_dqn_lr_const_avg, 'r:', alpha=0.5)\n",
    "plt.plot(13*np.ones((2400, )), 'k', linewidth=1.0)\n",
    "plt.ylim([0, 20])\n",
    "plt.xlim([0, 2400])\n",
    "plt.rc('grid', linestyle=\":\", color='black')\n",
    "plt.grid(True)\n",
    "plt.legend([\"DQN + Replay buffer (uniform choice) + decreasing learning rate\", \"DQN + Replay buffer (uniform choice) + const learning rate\"])\n",
    "#plt.legend([\"Dueling DDQN + Prioritised Replay\", \"DDQN + Prioritised Replay\", \"DQN + Replay buffer (uniform choice)\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(np.transpose(reward_mtx), interpolation='nearest', cmap=plt.cm.ocean)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(np.transpose(cum_reward_mtx), interpolation='nearest', cmap=plt.cm.ocean)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(np.transpose(cum_reward_mtx2), interpolation='nearest', cmap=plt.cm.ocean)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(np.transpose(cum_reward_mtx2 - cum_reward_mtx), interpolation='nearest', cmap=plt.cm.ocean)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "t = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('reward_matrix_{}.dat'.format(str(t.hour)+'_'+ str(t.minute)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(cum_reward_mtx[-1, :], \"k\")\n",
    "plt.plot(cum_reward_mtx[-2, :], \"r\")\n",
    "plt.plot(cum_reward_mtx[-3, :], \"b\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from pylab import *\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "\n",
    "#Create X and Y data\n",
    "x = np.arange(0, 300, 1)\n",
    "y = np.arange(0, 1500, 1)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "\n",
    "surf = ax.plot_surface(X, Y, cum_reward_mtx, rstride=1, cstride=1, antialiased=True)\n",
    "\n",
    "#Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
